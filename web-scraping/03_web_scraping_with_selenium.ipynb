{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Web scraping with selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Selenium basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve!\n",
      "A button to click!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "\n",
    "# chromedriver must be downloaded and located in same path\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/ajaxDemo.html\")\n",
    "time.sleep(3)\n",
    "\n",
    "print(driver.find_element_by_id(\"content\").text)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve!\n",
      "A button to click!\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/ajaxDemo.html\")\n",
    "\n",
    "try:\n",
    "    element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"loadedButton\")))\n",
    "finally:\n",
    "    print(driver.find_element_by_id(\"content\").text)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing out after 10 seconds and returning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<html><head>\\n<title>The Destination Page!</title>\\n\\n</head>\\n<body>\\nThis is the page you are looking for!\\n\\n</body></html>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "def wait_for_load(driver):\n",
    "    elem = driver.find_element_by_tag_name(\"html\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if count > 20:\n",
    "            print(\"Timing out after 10 seconds and returning\")\n",
    "            return\n",
    "        \n",
    "        time.sleep(.5)\n",
    "        try:\n",
    "            elem == driver.find_element_by_tag_name(\"html\")\n",
    "        except StaleElementReferenceException:\n",
    "            return\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/redirectDemo1.html\")\n",
    "\n",
    "wait_for_load(driver)\n",
    "driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping a shopping site with selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "\n",
    "# chromedriver must be downloaded and located in same path\n",
    "dr = webdriver.Chrome('./chromedriver', options=options)\n",
    "dr.get('http://store.musinsa.com/app/contents/onsale?d_cat_cd=&brand=&page_kind=onsale&list_kind=small&sort=pop&page=1')\n",
    "\n",
    "html = dr.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "sale_pdt = soup.find_all('p', attrs={'class': 'list_info'})\n",
    "sale_prc = soup.find_all('p', attrs={'class': 'price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt = []\n",
    "prc = []\n",
    "\n",
    "for i in range(0, len(sale_pdt)):\n",
    "    pdt.append(sale_pdt[i].find('a').text.strip())\n",
    "    prc.append(re.sub('[^0-9]','',sale_prc[i].text[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     product_name price\n",
      "0          오버사이즈 울 트렌치 코트 [BEIGE]      \n",
      "1                오버사이즈 모던 체크 블레이저      \n",
      "2  오버사이즈 울 트렌치 코트 [GREYISH BEIGE]      \n",
      "3                 엔젤 와펜 집업 후드 그레이      \n",
      "4          오버사이즈 울 트렌치 코트 [BLACK]      \n"
     ]
    }
   ],
   "source": [
    "data = {'product_name': pdt,\n",
    "        'price': prc}\n",
    "\n",
    "dt = DataFrame(data)\n",
    "print(dt.head())\n",
    "\n",
    "dt.to_csv('Musinsa_discount_{0}.csv'.format(datetime.today().strftime('%Y%m%d')),encoding='utf-8',index=False)\n",
    "dr.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scraping wikibooks site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_get_page(url):\n",
    "    try:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "\n",
    "        driver = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        driver.execute_script(\"document.getElementById('load-more').style.width = '100%';\")\n",
    "\n",
    "        count = 0\n",
    "        print(\"Scraping book list...\")\n",
    "        while driver.find_element_by_id(\"load-more\").text != \"더는 자료가 없습니다.\":\n",
    "            driver.find_element_by_css_selector(\"#load-more\").click()\n",
    "            count += 1\n",
    "            time.sleep(random.randint(1, 3))\n",
    "\n",
    "            print(\"Searching more books: {}\".format(count))\n",
    "        print(\"All book list saved!\")\n",
    "\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        return html\n",
    "\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return '0'\n",
    "\n",
    "def scrape_list_page(html_text):\n",
    "    soup = BeautifulSoup(html_text ,\"lxml\")\n",
    "    for li in soup.find('ul', {'id':'front-book-list'}).find_all('li', {'class':'book-in-front'}):\n",
    "        url = li.find('a').get('href')\n",
    "        yield url\n",
    "\n",
    "def scrape_detail_page(url):\n",
    "    session = requests.Session()\n",
    "    response = session.get(url).text\n",
    "\n",
    "    root = BeautifulSoup(response, \"lxml\")\n",
    "    normalize_space = lambda string: re.sub(r'\\s+', ' ', string).strip()\n",
    "    \n",
    "    if root:\n",
    "        try:\n",
    "            book_info = {\n",
    "                'url': url,\n",
    "                'title': root.find(\"div\", {'id':'content'}).find(\"h1\", {'class':'main-title'}).text,\n",
    "                'price': root.find(\"div\", {'id':'content'}).find(\"ul\", {'class':'book-info'}).find_all(\"li\")[4].text.split('|')[0].strip().replace('원', '').replace(',', ''),\n",
    "                'content': [normalize_space(p.text) for p in root.find(\"div\", {'id':'toc'}).find('ul').find_all('li') if normalize_space(p.text)]\n",
    "            }\n",
    "        except AttributeError:\n",
    "            book_info = None\n",
    "    \n",
    "    else:\n",
    "        book_info = None\n",
    "    \n",
    "    return book_info\n",
    "\n",
    "def json_save(file_name, data):\n",
    "    with open(file_name+\".json\", \"w\", encoding=\"UTF-8-sig\") as f:\n",
    "        json.dump(data, fp=f, ensure_ascii=False)\n",
    "    print(\"JSON data saved - filename: \" + file_name)\n",
    "\n",
    "def develop_data(html):\n",
    "    print(\"Scraping book info...\")\n",
    "    book_list = dict()\n",
    "    index = 0\n",
    "    for url in scrape_list_page(html):\n",
    "        book_info = scrape_detail_page(url)\n",
    "        if book_info:\n",
    "            book_list[str(index)] = book_info\n",
    "            index += 1\n",
    "            print(\"# of scraped book: {}\".format(index))\n",
    "        \n",
    "        time.sleep(random.randint(1, 3))\n",
    "    print(\"All book info saved!\")\n",
    "        \n",
    "    print(book_list)\n",
    "    json_save(\"wikibook_books\", book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping book list...\n",
      "Searching more books: 1\n",
      "Searching more books: 2\n",
      "Searching more books: 3\n",
      "Searching more books: 4\n",
      "Searching more books: 5\n",
      "Searching more books: 6\n",
      "Searching more books: 7\n",
      "Searching more books: 8\n",
      "Searching more books: 9\n",
      "Searching more books: 10\n",
      "Searching more books: 11\n",
      "Searching more books: 12\n",
      "Searching more books: 13\n",
      "Searching more books: 14\n",
      "Searching more books: 15\n",
      "Searching more books: 16\n",
      "Searching more books: 17\n",
      "Searching more books: 18\n",
      "All book list saved!\n",
      "Scraping book info...\n",
      "# of scraped book: 1\n",
      "# of scraped book: 2\n",
      "# of scraped book: 3\n",
      "# of scraped book: 4\n",
      "# of scraped book: 5\n",
      "# of scraped book: 6\n",
      "# of scraped book: 7\n",
      "# of scraped book: 8\n",
      "# of scraped book: 9\n",
      "# of scraped book: 10\n",
      "# of scraped book: 11\n",
      "# of scraped book: 12\n",
      "# of scraped book: 13\n",
      "# of scraped book: 14\n",
      "# of scraped book: 15\n",
      "# of scraped book: 16\n",
      "# of scraped book: 17\n",
      "# of scraped book: 18\n",
      "# of scraped book: 19\n",
      "# of scraped book: 20\n",
      "# of scraped book: 21\n",
      "# of scraped book: 22\n",
      "# of scraped book: 23\n",
      "# of scraped book: 24\n",
      "# of scraped book: 25\n",
      "# of scraped book: 26\n",
      "# of scraped book: 27\n",
      "# of scraped book: 28\n",
      "# of scraped book: 29\n",
      "# of scraped book: 30\n",
      "# of scraped book: 31\n",
      "# of scraped book: 32\n",
      "# of scraped book: 33\n",
      "# of scraped book: 34\n",
      "# of scraped book: 35\n",
      "# of scraped book: 36\n",
      "# of scraped book: 37\n",
      "# of scraped book: 38\n",
      "# of scraped book: 39\n",
      "# of scraped book: 40\n",
      "# of scraped book: 41\n",
      "# of scraped book: 42\n",
      "# of scraped book: 43\n",
      "# of scraped book: 44\n",
      "# of scraped book: 45\n",
      "# of scraped book: 46\n",
      "# of scraped book: 47\n",
      "# of scraped book: 48\n",
      "# of scraped book: 49\n",
      "# of scraped book: 50\n",
      "# of scraped book: 51\n",
      "# of scraped book: 52\n",
      "# of scraped book: 53\n",
      "# of scraped book: 54\n",
      "# of scraped book: 55\n",
      "# of scraped book: 56\n",
      "# of scraped book: 57\n",
      "# of scraped book: 58\n",
      "# of scraped book: 59\n",
      "# of scraped book: 60\n",
      "# of scraped book: 61\n",
      "# of scraped book: 62\n",
      "# of scraped book: 63\n",
      "# of scraped book: 64\n",
      "# of scraped book: 65\n",
      "# of scraped book: 66\n",
      "# of scraped book: 67\n",
      "# of scraped book: 68\n",
      "# of scraped book: 69\n",
      "# of scraped book: 70\n",
      "# of scraped book: 71\n",
      "# of scraped book: 72\n",
      "# of scraped book: 73\n",
      "# of scraped book: 74\n",
      "# of scraped book: 75\n",
      "# of scraped book: 76\n",
      "# of scraped book: 77\n",
      "# of scraped book: 78\n",
      "# of scraped book: 79\n",
      "# of scraped book: 80\n",
      "# of scraped book: 81\n",
      "# of scraped book: 82\n",
      "# of scraped book: 83\n",
      "# of scraped book: 84\n",
      "# of scraped book: 85\n",
      "# of scraped book: 86\n",
      "# of scraped book: 87\n",
      "# of scraped book: 88\n",
      "# of scraped book: 89\n",
      "# of scraped book: 90\n",
      "# of scraped book: 91\n",
      "# of scraped book: 92\n",
      "# of scraped book: 93\n",
      "# of scraped book: 94\n",
      "# of scraped book: 95\n",
      "# of scraped book: 96\n",
      "# of scraped book: 97\n",
      "# of scraped book: 98\n",
      "# of scraped book: 99\n",
      "# of scraped book: 100\n",
      "# of scraped book: 101\n",
      "# of scraped book: 102\n",
      "# of scraped book: 103\n",
      "# of scraped book: 104\n",
      "# of scraped book: 105\n",
      "# of scraped book: 106\n",
      "# of scraped book: 107\n",
      "# of scraped book: 108\n",
      "# of scraped book: 109\n",
      "# of scraped book: 110\n",
      "# of scraped book: 111\n",
      "# of scraped book: 112\n",
      "# of scraped book: 113\n",
      "# of scraped book: 114\n",
      "# of scraped book: 115\n",
      "# of scraped book: 116\n",
      "# of scraped book: 117\n",
      "# of scraped book: 118\n",
      "# of scraped book: 119\n",
      "# of scraped book: 120\n",
      "# of scraped book: 121\n",
      "# of scraped book: 122\n",
      "# of scraped book: 123\n",
      "# of scraped book: 124\n",
      "# of scraped book: 125\n",
      "# of scraped book: 126\n",
      "# of scraped book: 127\n",
      "# of scraped book: 128\n",
      "# of scraped book: 129\n",
      "# of scraped book: 130\n",
      "# of scraped book: 131\n",
      "# of scraped book: 132\n",
      "# of scraped book: 133\n",
      "# of scraped book: 134\n",
      "# of scraped book: 135\n",
      "# of scraped book: 136\n",
      "# of scraped book: 137\n",
      "# of scraped book: 138\n",
      "# of scraped book: 139\n",
      "# of scraped book: 140\n",
      "# of scraped book: 141\n",
      "# of scraped book: 142\n",
      "# of scraped book: 143\n",
      "# of scraped book: 144\n",
      "# of scraped book: 145\n",
      "# of scraped book: 146\n",
      "# of scraped book: 147\n",
      "# of scraped book: 148\n",
      "# of scraped book: 149\n",
      "# of scraped book: 150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f720ea6f7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No data found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '<html lang=\"ko-KR\" style=\"height: 100%;\"><!--<![endif]--><head><script type=\"text/javascript\" src=\"https://m.addthis.com/live/red_lojson/300lo.json?si=6130b11b69e7b3d0&amp;bkl=0&amp;bl=1&amp;pdt=810&",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f720ea6f7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No data found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdevelop_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0e974a46f6ff>\u001b[0m in \u001b[0;36mdevelop_data\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# of scraped book: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All book info saved!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "html = dynamic_get_page(\"http://wikibook.co.kr/\")\n",
    "\n",
    "try:\n",
    "    int(html)\n",
    "    print('No data found')\n",
    "except:\n",
    "    develop_data(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
