{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tutorial\n",
    "Reference: <a href=\"https://github.com/savan77/Exploring-NLTK\">Github site</a><br>\n",
    "\n",
    "If want more information of NLTK ...<br>\n",
    "Book: <a href=\"http://www.nltk.org/book/\">Natural Language Processing with Python</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m/UNKNOWN\n",
      "sysconfig: /Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 738 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk) (0.13.2)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk) (4.33.0)\n",
      "Installing collected packages: nltk\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m/UNKNOWN\n",
      "sysconfig: /Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Successfully installed nltk-3.6.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/a10053/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/a10053/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/a10053/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/a10053/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/a10053/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/a10053/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read txt data\n",
    "corpus = open('./nltk_data/my_text.txt', 'r').read()\n",
    "\n",
    "# Check corpus type\n",
    "type(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A ``knowledge engineer'' interviews experts in a certain domain and tries to embody their knowledge in a computer program for carrying out some task.\", 'How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI.', 'When this turned out not to be so, there were many disappointing results.', 'One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments.', 'It did better than medical students or practicing doctors, provided its limitations were observed.', 'Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doctors, hospitals, death, recovery, and events occurring in time.', 'Its interactions depended on a single patient being considered.', 'Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework.', 'In the present state of AI, this has to be true.', 'The usefulness of current expert systems depends on their users having common sense.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '``', 'knowledge', 'engineer', \"''\", 'interviews', 'experts', 'in', 'a', 'certain', 'domain', 'and', 'tries', 'to', 'embody', 'their', 'knowledge', 'in', 'a', 'computer', 'program', 'for', 'carrying', 'out', 'some', 'task', '.', 'How', 'well', 'this', 'works', 'depends', 'on', 'whether', 'the', 'intellectual', 'mechanisms', 'required', 'for', 'the', 'task', 'are', 'within', 'the', 'present', 'state', 'of', 'AI', '.', 'When', 'this', 'turned', 'out', 'not', 'to', 'be', 'so', ',', 'there', 'were', 'many', 'disappointing', 'results', '.', 'One', 'of', 'the', 'first', 'expert', 'systems', 'was', 'MYCIN', 'in', '1974', ',', 'which', 'diagnosed', 'bacterial', 'infections', 'of', 'the', 'blood', 'and', 'suggested', 'treatments', '.', 'It', 'did', 'better', 'than', 'medical', 'students', 'or', 'practicing', 'doctors', ',', 'provided', 'its', 'limitations', 'were', 'observed', '.', 'Namely', ',', 'its', 'ontology', 'included', 'bacteria', ',', 'symptoms', ',', 'and', 'treatments', 'and', 'did', 'not', 'include', 'patients', ',', 'doctors', ',', 'hospitals', ',', 'death', ',', 'recovery', ',', 'and', 'events', 'occurring', 'in', 'time', '.', 'Its', 'interactions', 'depended', 'on', 'a', 'single', 'patient', 'being', 'considered', '.', 'Since', 'the', 'experts', 'consulted', 'by', 'the', 'knowledge', 'engineers', 'knew', 'about', 'patients', ',', 'doctors', ',', 'death', ',', 'recovery', ',', 'etc.', ',', 'it', 'is', 'clear', 'that', 'the', 'knowledge', 'engineers', 'forced', 'what', 'the', 'experts', 'told', 'them', 'into', 'a', 'predetermined', 'framework', '.', 'In', 'the', 'present', 'state', 'of', 'AI', ',', 'this', 'has', 'to', 'be', 'true', '.', 'The', 'usefulness', 'of', 'current', 'expert', 'systems', 'depends', 'on', 'their', 'users', 'having', 'common', 'sense', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "words = nltk.word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# push stopwords to s_words\n",
    "s_words = stopwords.words('english')\n",
    "\n",
    "# print head of stopwords\n",
    "print(stopwords.words('english')[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords from a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '``', 'knowledge', 'engineer', \"''\", 'interviews', 'experts', 'certain', 'domain', 'tries', 'embody', 'knowledge', 'computer', 'program', 'carrying', 'task', '.', 'How', 'well', 'works', 'depends', 'whether', 'intellectual', 'mechanisms', 'required', 'task', 'within', 'present', 'state', 'AI', '.', 'When', 'turned', ',', 'many', 'disappointing', 'results', '.', 'One', 'first', 'expert', 'systems', 'MYCIN', '1974', ',', 'diagnosed', 'bacterial', 'infections', 'blood', 'suggested', 'treatments', '.', 'It', 'better', 'medical', 'students', 'practicing', 'doctors', ',', 'provided', 'limitations', 'observed', '.', 'Namely', ',', 'ontology', 'included', 'bacteria', ',', 'symptoms', ',', 'treatments', 'include', 'patients', ',', 'doctors', ',', 'hospitals', ',', 'death', ',', 'recovery', ',', 'events', 'occurring', 'time', '.', 'Its', 'interactions', 'depended', 'single', 'patient', 'considered', '.', 'Since', 'experts', 'consulted', 'knowledge', 'engineers', 'knew', 'patients', ',', 'doctors', ',', 'death', ',', 'recovery', ',', 'etc.', ',', 'clear', 'knowledge', 'engineers', 'forced', 'experts', 'told', 'predetermined', 'framework', '.', 'In', 'present', 'state', 'AI', ',', 'true', '.', 'The', 'usefulness', 'current', 'expert', 'systems', 'depends', 'users', 'common', 'sense', '.']\n"
     ]
    }
   ],
   "source": [
    "# save words not in stopwords to filtered_corpus\n",
    "filtered_corpus = [w for w in words if not w in s_words]\n",
    "print(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "# if # of filtered_corpus is lower than words\n",
    "print(len(filtered_corpus) < len(words))\n",
    "\n",
    "# # of removed words\n",
    "print(len(words) - len(filtered_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging<br>\n",
    "Have a look at  - <a href=\"http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\"> List of POS Tags </a> . To know more about these words (NN, RB, VBZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('``', '``'), ('knowledge', 'NN'), ('engineer', 'NN'), (\"''\", \"''\"), ('interviews', 'NNS'), ('experts', 'NNS'), ('certain', 'JJ'), ('domain', 'NN'), ('tries', 'NNS'), ('embody', 'VBP'), ('knowledge', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('carrying', 'NN'), ('task', 'NN'), ('.', '.'), ('How', 'WRB'), ('well', 'RB'), ('works', 'VBZ'), ('depends', 'VBZ'), ('whether', 'IN'), ('intellectual', 'JJ'), ('mechanisms', 'NNS'), ('required', 'VBN'), ('task', 'NN'), ('within', 'IN'), ('present', 'JJ'), ('state', 'NN'), ('AI', 'NNP'), ('.', '.'), ('When', 'WRB'), ('turned', 'VBD'), (',', ','), ('many', 'JJ'), ('disappointing', 'JJ'), ('results', 'NNS'), ('.', '.'), ('One', 'CD'), ('first', 'JJ'), ('expert', 'JJ'), ('systems', 'NNS'), ('MYCIN', 'NNP'), ('1974', 'CD'), (',', ','), ('diagnosed', 'VBD'), ('bacterial', 'JJ'), ('infections', 'NNS'), ('blood', 'NN'), ('suggested', 'VBD'), ('treatments', 'NNS'), ('.', '.'), ('It', 'PRP'), ('better', 'RBR'), ('medical', 'JJ'), ('students', 'NNS'), ('practicing', 'VBG'), ('doctors', 'NNS'), (',', ','), ('provided', 'VBN'), ('limitations', 'NNS'), ('observed', 'VBN'), ('.', '.'), ('Namely', 'RB'), (',', ','), ('ontology', 'NN'), ('included', 'VBD'), ('bacteria', 'NNS'), (',', ','), ('symptoms', 'NNS'), (',', ','), ('treatments', 'NNS'), ('include', 'VBP'), ('patients', 'NNS'), (',', ','), ('doctors', 'NNS'), (',', ','), ('hospitals', 'NNS'), (',', ','), ('death', 'NN'), (',', ','), ('recovery', 'NN'), (',', ','), ('events', 'NNS'), ('occurring', 'VBG'), ('time', 'NN'), ('.', '.'), ('Its', 'PRP$'), ('interactions', 'NNS'), ('depended', 'VBN'), ('single', 'JJ'), ('patient', 'NN'), ('considered', 'VBN'), ('.', '.'), ('Since', 'IN'), ('experts', 'NNS'), ('consulted', 'VBN'), ('knowledge', 'NN'), ('engineers', 'NNS'), ('knew', 'VBD'), ('patients', 'NNS'), (',', ','), ('doctors', 'NNS'), (',', ','), ('death', 'NN'), (',', ','), ('recovery', 'NN'), (',', ','), ('etc.', 'FW'), (',', ','), ('clear', 'JJ'), ('knowledge', 'NN'), ('engineers', 'NNS'), ('forced', 'VBD'), ('experts', 'NNS'), ('told', 'VBD'), ('predetermined', 'VBN'), ('framework', 'NN'), ('.', '.'), ('In', 'IN'), ('present', 'JJ'), ('state', 'NN'), ('AI', 'NNP'), (',', ','), ('true', 'JJ'), ('.', '.'), ('The', 'DT'), ('usefulness', 'JJ'), ('current', 'JJ'), ('expert', 'NN'), ('systems', 'NNS'), ('depends', 'VBZ'), ('users', 'JJ'), ('common', 'JJ'), ('sense', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging with filtered_corpus\n",
    "tags = nltk.pos_tag(filtered_corpus)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "# 위에서 썼다면 생략 가능\n",
    "# from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# pattern to detect noun phrases\n",
    "pattern = \"\"\"\n",
    "    NP: {<JJ>*<NN>+}\n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\"\n",
    "\n",
    "chunker = RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "he National Wrestling Association was an early professional wrestling sanctioning body created in 1930 by \n",
    "the National Boxing Association (NBA) (now the World Boxing Association, WBA) as an attempt to create\n",
    "a governing body for professional wrestling in the United States. The group created a number of \"World\" level \n",
    "championships as an attempt to clear up the professional wrestling rankings which at the time saw a number of \n",
    "different championships promoted as the \"true world championship\". The National Wrestling Association's NWA \n",
    "World Heavyweight Championship was later considered part of the historical lineage of the National Wrestling \n",
    "Alliance's NWA World Heavyweight Championship when then National Wrestling Association champion Lou Thesz \n",
    "won the National Wrestling Alliance championship, folding the original championship into one title in 1949.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into sentences\n",
    "tokenized_sentence = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize words in sentences.\n",
    "tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]  \n",
    "\n",
    "# Tag words for POS in each sentence.\n",
    "tagged_words = [nltk.pos_tag(word) for word in tokenized_words]\n",
    "\n",
    "# Identify NP chunks\n",
    "word_tree = [chunker.parse(word) for word in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the tree\n",
    "word_tree[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  studies/NNS\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "sentence = \"John studies at Stanford University.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"makes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grow'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another stemmer is snowball\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "stemmer2.stem(\"grows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"makes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('man.n.01'),\n",
       " Synset('serviceman.n.01'),\n",
       " Synset('man.n.03'),\n",
       " Synset('homo.n.02'),\n",
       " Synset('man.n.05'),\n",
       " Synset('man.n.06'),\n",
       " Synset('valet.n.01'),\n",
       " Synset('man.n.08'),\n",
       " Synset('man.n.09'),\n",
       " Synset('man.n.10'),\n",
       " Synset('world.n.08'),\n",
       " Synset('man.v.01'),\n",
       " Synset('man.v.02')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn.synsets('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'someone who serves in the armed forces; a member of a military force'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('man')[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog barked all night'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "dog.examples()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"Science is the methodical study of nature including testable explanations and predictions. From classical antiquity through the 19th century, science as a type of knowledge was more closely linked to philosophy than it is now and, in fact, in the Western world, the term \"natural philosophy\" encompassed fields of study that are today associated with science, such as astronomy, medicine, and physics. However, during the Islamic Golden Age foundations for the scientific method were laid by Ibn al-Haytham in his Book of Optics. While the classification of the material world by the ancient Indians and Greeks into air, earth, fire and water was more philosophical, medieval Middle Easterns used practical, experimental observation to classify materials.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 12, 'the': 9, 'of': 5, 'and': 5, '.': 4, 'in': 3, 'is': 2, 'study': 2, 'science': 2, 'as': 2, ...})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fd = FreqDist(nltk.word_tokenize(t))\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
